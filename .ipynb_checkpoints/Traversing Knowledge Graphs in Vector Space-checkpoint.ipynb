{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Traversing Knowledge Graphs in Vector Space](https://arxiv.org/abs/1506.01094)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "A **knowledge graph** such as [Freebase](http://dl.acm.org/citation.cfm?doid=1376616.1376746) or [Wordnet](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.105.1244) consists of a set of **entities** (nodes) and a set of **binary relations** (edges) between entities. \n",
    "\n",
    "One of their most powerful aspects is that they support **compositional queries**. For example, compositional questions like \"What languages are spoken by people of Portugal?\" can be converted to a path query - (portugal/location/language), which can be then, executed on the knowledge graph using graph traversal.\n",
    "\n",
    "<img src=\"KB_graph_traversal.png\" style=\"float: left;\" alt=\"Knowledge Base Graph Traversal\" title=\"Knowledge Base Graph Traversal\" height=400 width=400 />\n",
    "<img src=\"KB_incomplete.png\" alt=\"Incomplete Knowledge Base\" title=\"Incomplete Knowledge Base\" height=400 width=400 />\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> However, kowledge graphs are known to suffer from incomplete coverage\n",
    "(<a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.367.2851\">Min et al., 2013</a>).</div>\n",
    "\n",
    "For example, in the left sub-graph, we might not know the nationality of Fernando Pessoa, leaving the knowledge graph incomplete. That **fact** can be represented as a **triple** (subject, predicate/relation, object), which can be represented as an  **edge** in the knowledge graph.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> Knowledge graph's inherent strength is their compositionality with the added weakness of their incompleteness. </div>\n",
    "\n",
    "An elegant solution to incompleteness is using vector space representations, i.e. to embedd the entire knowledge base into vector space by representing each of the entities as points in low dimensional space and relationships between them are reflected in the spatial relationships between those points. \n",
    "\n",
    "<img src=\"Vector_Space_Model.png\" alt=\"Knowledge Base Vector Space Model\" title=\"Knowledge Base Vector Space Model\" height=500 width=500 />\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> Sequeezing large number of facts in low dimensional space compresses knowledge in a new way that forces it to generalize better to new facts. \n",
    "(<a href=\"https://dl.acm.org/citation.cfm?id=3104584\">Tensor Factorization {Nickel et al., 2011};</a>\n",
    " <a href=\"https://dl.acm.org/citation.cfm?id=2999715\">Neural Tensor Network {Socher et al., 2013};</a>\n",
    "<a href=\"http://www.aclweb.org/anthology/N13-1008\">Universal Schema {Riedel et al., 2013};</a>\n",
    "<a href=\"http://www.aclweb.org/anthology/P15-1016\">Compositional Embedding of Paths {Neelakantan et al., 2015}</a>). </div>\n",
    "\n",
    "This work has two key empirical findings:\n",
    "1. Compositional queries can also be done in vector space despite the fact that there is inherent error introduced through compression as compositional training enables us to answer path queries by substantially reducing cascading errors present in the base vector space model. \n",
    "2. Compositional training also improves upon state-of-the-art performance for knowledge base completion, i.e. handling incomplete unit length queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compositionalization\n",
    "\n",
    "In **graph database**, path queries are solved using recursive path traversal. Starting from the anchor entity (*Portugal*), create the next set of entities (*people of Portugal*) by traversing the next relational edge (*location*) and using the entities in current set (*Fernando Pessoa, Jorge Sampio ...*) recursively traverse to next entity set until we have traversed the final relational edge (*language*), which will give us the solution set (*Portuguese, Spanish, English*). This is equivalent of **adjacency matrix multiplication** corresponding to the path relation with the **sparse one-hot vector** of the starting entity node in the graph. Given an entity (*English*), we can it quickly check if it is contained in the resulting set by **dot product** between the set vector and one-hot vecotr.\n",
    "\n",
    "Now, take the above adjacency matrix and compress it to lower dimensional **compressed adjaceny matrix** and the sparse one-hot vectors to **dense low dimensional vectors**.  \n",
    "\n",
    "In **vector space model**, sets of entities are represented by real vectors. Transformation between sets is performed by a vector-to-vector transformation like matrix multiplication. This allows us to start with a set consisting of a single entity (*Portugal*), sequentially apply **traversal operators** (*location,language*), and end with a set representing the answer (*Portuguese, Spanish, English*). Given an entity (*English*), we can it quickly check if it is contained in this set using a **membership operator.**\n",
    "\n",
    "<img src=\"Vector_Space_Path_Query.png\" alt=\"Vector Space Path Query\" title=\"Vector Space Path Query\" height=500 width=500 />\n",
    "\n",
    "Given a query $q = s/r_1/r_2/\\cdots/r_k$, where $s$ is anchor entity s and $r_1/r_2/\\cdots/r_k$ are the sequence of relations, we wish to evaluate.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> This path traversal can be done as $x_s^\\top W_{r_{1}} \\cdots W_{r_{k}}$ and to check, how likely given $t$ is the answer to $q$, evaluate a membership score,\n",
    "$\\mathrm{score}(q, t) =  x_s^\\top W_{r_{1}} \\cdots W_{r_{k}} x_t$. </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization\n",
    "\n",
    "The works in [Tensor Factorization (Nickel et al., 2011);](https://dl.acm.org/citation.cfm?id=3104584)\n",
    "[Neural Tensor Network (Socher et al., 2013);](https://dl.acm.org/citation.cfm?id=2999715)\n",
    "[Universal Schema (Riedel et al., 2013);](http://www.aclweb.org/anthology/N13-1008)\n",
    "[Compositional Embedding of Paths (Neelakantan et al., 2015)](http://www.aclweb.org/anthology/P15-1016) all define a score function on (subject, predicate, object) triples and all these models are trained to assign **high scores to true triples (facts)** and vice-versa.\n",
    "\n",
    "For example,\n",
    "\n",
    "score(subject, predicate, object)\n",
    "<br><font color=green>score(Obama, nationality, United States) = HIGH </font>\n",
    "<br><font color=red>score(Obama, nationality, Portugal) = LOW </font> \n",
    "\n",
    "In all the above papers, the way to predict missing facts during inference is given an unknown triple, if the model gives high score, the triple is accepted as fact with missing edge in the knowledge base, which are then added to the knowledge base. But these all are just special case of unit length path queries.\n",
    "\n",
    "All the above models are generalized in this work from going from single hop to multi-hop path queries:\n",
    "\n",
    "<img src=\"Path_Traversal_Generalizations.png\" alt=\"Vector Space Path Query Generalization\" title=\"Vector Space Path Query Generalization\" height=500 width=500 />\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "More generally, taking any triple scoring function, that factorises into membership operator $ \\mathbb{M}: \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}$ and traversal operator $ \\mathbb{T}_r : \\mathbb{R}^d \\to \\mathbb{R}^d$, can be unrolled to multiple traversal operators for path queries. \n",
    "</div>\n",
    "\n",
    "This generalization of knowledge base completion models helps in two ways:\n",
    "1. For **inference**, we can now answer path queries instead of just single hop queries. \n",
    "2. For **training**, we can now train directly on path data instead of binary classification on single edges, i.e. we can train the model to acknowledge longer range relationships. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "The authors perform experiments on three different embedding models. \n",
    "Given a query $q = s/r_1/r_2/\\cdots/r_k$ and an answer $t$, each of the models assigns a score to how likely $t$ is the answer to $q$. \n",
    "In all of the models, each entity $e$ is represented as a vector $x_e \\in \\mathbb{R}^d$.\n",
    "\n",
    "1. **TransE** : In transE, each relation $r$ is represented by a vector $w_r \\in \\mathbb{R}^d$. The score function is given by\n",
    "$\\mathrm{score}(q, t) = -\\| x_{s} + w_{r_1} + \\cdots + w_{r_k} - x_{t} \\|_2^2$.\n",
    "\n",
    "2. **Bilinear** : In the bilinear model, each relation $r$ is represented by a matrix $W_r \\in \\mathbb{R}^{d \\times d}$. The score function is given by\n",
    "$\\mathrm{score}(q, t) = x_s^\\top W_{r_{1}} \\cdots W_{r_{k}} x_t$.\n",
    "\n",
    "3. **Bilinear-Diag** : In bilinear-diag, each relation $r$ is represented by a diagonal matrix $\\mathop{\\bf diag}\\left(w_r\\right) \\in \\mathbb{R}^{d \\times d}$. The score function is given by\n",
    "$\\mathrm{score}(q, t) = x_s^\\top \\mathop{\\bf diag}\\left(w_{r_{1}}\\right) \\cdots \\mathop{\\bf diag}\\left(w_{r_{k}}\\right) x_t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "- Training Examples  :  $(q,t)$\n",
    "- Margin : $\\text{margin}(q, t, t') = \\mathrm{score}(q, t) - \\mathrm{score}(q, t')$\n",
    "- Objective : $\\sum_{i =1}^N \\sum_{t' \\in \\mathcal{N}(q_i)} \\left[1 - \\mathrm{margin}(q_i, t_i, t')\\right]_{+}$\n",
    "- Algorithm : **Adagrad**\n",
    "\n",
    "We optimize the following max-margin objective using Adagrad\n",
    "$J(\\Theta) = \\sum_{i =1}^N \\sum_{t' \\in \\mathcal{N}(q_i)} \\left[1 - \\mathrm{margin}(q_i, t_i, t')\\right]_{+}$,\n",
    "where $\\text{margin}(q, t, t') = \\mathrm{score}(q, t) - \\mathrm{score}(q, t')$ and $\\mathcal{N}(q_i)$ is the set of incorrect answers to the query, i.e. maximizing the margin between membership scores of correct and incorrect answers.\n",
    "We employ a curriculum learning strategy. We first train on single edges (path queries of length 1) until convergence and then train on the full path query dataset until convergence. This ensures the model masters single edges before seeking to compose them into paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "For all of our experiments, we use the subsets of Wordnet and Freebase evaluated\n",
    "in [Socher et al. (2013)](http://nlp.stanford.edu/~socherr/SocherChenManningNg_NIPS2013.pdf). These datasets consist only of single-edge queries, so they are referred to as the **base datasets.**\n",
    "\n",
    "From the base Wordnet and Freebase subsets provided by [Socher et al. (2013)](http://nlp.stanford.edu/~socherr/SocherChenManningNg_NIPS2013.pdf), we generate\n",
    "path queries by performing path-sampling (random walks on the knowledge graph). These datasets are referred to as the **path-query datasets**.\n",
    "\n",
    "<img style=\"float: left;\" src=\"Datasets.png\" alt=\"Path Query Datasets\" title=\"Path Query Datasets\" height=300 width=300 />\n",
    "<img src=\"Path_Sampling.png\" alt=\"Path Sampling\" title=\"Path Sampling\" height=300 width=300 />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "In our experiments, we evaluate knowledge base queries using **Hits at 10 (h@10)** and mean quantile (MQ). Hits at 10 is the percentage of correct answers ranked in the top 10, and mean quantile is a normalized version of mean rank. For a query $q$ and an correct answer $t$, the mean quantile is percentage of incorrect answers ranked after $t$ (100 is the best i.e. all incorrects after the correct answers and similarly, 0 is the worst). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "<img src=\"Results_Table.png\" alt=\"Results Table\" title=\"Results Table\" height=500 width=500 />\n",
    "\n",
    "The upper half of the table shows that compositional training improves path querying performance across all models and metrics on both datasets, reducing error by up to 76.2%.\n",
    "The lower half of Table 2 shows that surprisingly, compositional training also improves performance on knowledge base completion across almost all models, metrics and datasets.\n",
    "\n",
    "<img style=\"float: left;\" src=\"Results_Path_Query.png\" alt=\"Results for Path Querying Graph\" title=\"Results for Path Querying Graph\" height=350 width=350 />\n",
    "<img src=\"Results_KBC.png\" alt=\"Results for Knowledge Base Completion Graph\" title=\"Results for Knowledge Base Completion Graph\" height=350 width=350 />\n",
    "\n",
    "Plotting them as barcharts, the two bars in both charts for path querying tasks and knowledge base completion (single path querying) tasks represent the MQ score of the single edge training (**Edge**) and the path training (**Path**) respectively for the corresponding models and the datasets on path query preformance task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis & Conclusions\n",
    "\n",
    "### Why does compositional path training improve path query answering?\n",
    "\n",
    "<img src=\"Cascading_Errors.png\" style=\"float: left;\" alt=\"Cascading Errors\" title=\"Cascading Errors\" height=350 width=400 />\n",
    "<img src=\"Reconstruction_Quality.png\" alt=\"Reconstruction Quality\" title=\"Reconstruction Quality\" height=300 width=400 /> \n",
    "\n",
    "Single edge’s relatively weak performance on the path query dataset is due to cascading errors along the path. For a\n",
    "given edge $(s, r, t)$ on the path, single-edge training encourages $x_t$ to be closer to $x_{s}^{T}W_r$ than any other incorrect $x_{t'}$. However, once this is achieved by a margin of 1, it does not push $x_t$ any closer to $x_{s}^{T}W_r$. The remaining discrepancy is noise which gets added at each step of path traversal, as illustrated with an example in the left figure.\n",
    "\n",
    "To observe this phenomenon empirically, we examine how well a model handles each intermediate step of a path query. We can do this by measuring the reconstruction quality (RQ) of the set vector produced after each traversal operation as in right figure.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> Single edge training leads to cascading errors while answering path queries, which gets substaintially cut down by path training.</div>\n",
    "\n",
    "### Why does compositional path training improve knowledge base completion?\n",
    "\n",
    "Paths in a knowledge graph have proven to be important features for predicting the existence of single edges\n",
    "([Lao et al., 2011](https://dl.acm.org/citation.cfm?id=2145494); [Neelakantan et al., 2015](http://www.aclweb.org/anthology/P15-1016)).\n",
    "\n",
    "For example, consider in the following knowledge base sub-graph, where there is missing edge between A and C. The path between A to C helps infer about the missing single relation edge (child) between A and C. This can be written as Horn Clause:\n",
    "\n",
    "<img src=\"Child.png\" alt=\"Child\" title=\"Child\" height=100 width=100 />\n",
    "\n",
    "$spouse (A, B) \\wedge child (B, C) \\implies child (A,C)$\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> Path training implicitly learns and exploits Horn clauses. </div>\n",
    "\n",
    "### Conclusions\n",
    "Intuitively, one way for the model to implicitly learn and exploit a Horn clause would be to satisfy the following two criteria, which path training does significantly better than edge training:\n",
    "1. The model ensures a consistent spatial relationship between entity pairs that are related by the path type p; that is, keeping $x_s^\\top W_{r_{1}} \\cdots W_{r_{k}}$ close to $x_t$ for all valid $(s, t)$ pairs.\n",
    "2. The model’s representation of the path type p and relation r captures that spatial relationship; that is, $x_s^\\top W_{r_{1}} \\cdots W_{r_{k}} \\equiv x_t \\implies x_s^\\top W_{r} \\equiv x_t$, or simply $W_{r_{1}} \\cdots W_{r_{k}} \\equiv W_{r}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments and Proposed Improvements\n",
    "\n",
    "1. The model only captures linear relationships by translation operators in vectors space. The improvement could be, if the model can also account for complex relationships in vector space, which are not basic linear translations.\n",
    "2. The model does not answer how to find the triples for knowledge base completion query i.e. it actually enumerates all the possible triples and checks the membership score on them.\n",
    "3. The model can further, be interpolated for neural nets as repeated application of traversal operator can be thoughts as RNNs.\n",
    "4. Path training on word embeddings can also find hidden complex relationships between words, which can be then semantically used and evaluated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
